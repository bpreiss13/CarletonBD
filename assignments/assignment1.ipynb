{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **DO NOT EDIT IF INSIDE `CarletonBD` folder** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handin in Peergrade**: *Wednesday*, February 3, 2021, 23:59<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex 1.1.12**: Take another list `b = list(\"ofcourseistillloveyou\")` and\n",
    "1. get the `set` of characters that exist in both `a` and `b` (intersection),\n",
    "2. get the `set` of characters that exist in either `a` or `b` (union), and\n",
    "3. compute the [Jaccard similarity](https://en.wikipedia.org/wiki/Jaccard_index) between the distinct elements in `a` and `b`.\n",
    ">\n",
    ">*Hint: use the `set` function to get a `set`-type object of distinct elements from a list. Sets supports a [number of different operations](https://snakify.org/en/lessons/sets/#section_4).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer to 1.1.12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 1.2.4**: The URL reveals that the data is from reddit/r/gameofthrones, but can you recover that information from the data? Give your answer by 'keying' into the dictionary using square brackets.\n",
    ">\n",
    ">*Hint: 'Keying' is a word i just made up. By it, I mean the following. Consider a nested dictionary like:*\n",
    ">\n",
    ">        my_json_obj = {\n",
    ">            'cats': {\n",
    ">                'awesome': ['Missy'],\n",
    ">                'useless': ['Kim', 'Frank', 'Sandy']\n",
    ">            },\n",
    ">            'dogs': {\n",
    ">                'awesome': ['Finn', 'Dolores', 'Fido', 'Casper'],\n",
    ">                'useless': []\n",
    ">            }\n",
    ">        }\n",
    ">\n",
    ">*I can get the list of useless cats by keying into `my_json_obj` like such:*\n",
    ">\n",
    ">        >>> my_json_obj['cats']['useless']\n",
    ">        Out [ ]: ['Kim', 'Frank', 'Sandy']\n",
    ">\n",
    ">*`my_json_obj['cats']` returns the dictionary `{'awesome': ['Missy'], 'useless': ['Kim', 'Frank', 'Sandy']}` and getting '`useless`' from that eventually gives us `['Kim', 'Frank', 'Sandy']`. If any of those list items were a list of a dictionary themselves, we could have kept keying deeper into the structure.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 1.2.4 -- make sure the code here has everything it needs to run by itself (import statements, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex 1.2.5**: Write two `for` loops (or list comprehensions for extra street credits) which:\n",
    ">1. Counts the number of spoilers.\n",
    ">2. Only prints headlines that aren't spoilers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 1.2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.1.2**: The `get_x_y` function below gives you the number of comments versus score for the latest `N` posts on a given `subreddit`.\n",
    "1. Make a scatter plot of `x` vs. `y` for the \"blackmirror\" subreddit (**remember** what you learned in the previous exercise about **styling**). Comment on what you see.\n",
    "2. Maybe you've noticed that it looks pretty bad right? That's because the data does *not scale linearly*! This is a very common thing. To visualize it you should then try to *transform* it somehow. In this case, the data scales *exponentially* in both the x and y direction. Which transformation should we use to make it linear?\n",
    "3. In two separate figures, floating side by side, scatter plot (left) the set of x and y variables for \"blackmirror\" and (right) x and y for \"news\". Remember to transform the data. My figure looks like [this](http://ulfaslak.com/computational_analysis_of_big_data/exer_figures/example_2.2b.png).\n",
    "4. Comment on any differences you see in the trends. Why might number of comments versus post upvotes look different for a TV-show than for world news?\n",
    ">\n",
    ">*Hint: By \"transformation\" I explicitly mean that you map some function onto every value in a list of values. Example: I can apply a square root transformation like `x = [np.sqrt(v) for v in x]`. A faster way to do that, of course, would be just `x = np.sqrt(x)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T21:28:54.941490Z",
     "start_time": "2019-01-23T21:28:44.166852Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "\n",
    "def get_x_y(subreddit, N, count=25):\n",
    "    \n",
    "    def _get_data(subreddit, count, after):\n",
    "        url = \"https://www.reddit.com/r/%s/.json?count=%d&after=%s\" % (subreddit, count, after)\n",
    "        data = rq.get(url, headers = {'User-agent': 'sneakybot'}).json()\n",
    "        print(\"Retrieved %d posts from page %s\" % (count, after))\n",
    "        return data\n",
    "    \n",
    "    after = \"\"\n",
    "\n",
    "    x, y = [], []\n",
    "    for n in range(N//count):\n",
    "        data = _get_data(subreddit, count, after)\n",
    "        for d in data['data']['children']:\n",
    "            x.append(d['data']['num_comments'])\n",
    "            y.append(d['data']['score'])\n",
    "        after = data['data']['after']\n",
    "\n",
    "    return x, y\n",
    "                          \n",
    "x, y = get_x_y(\"blackmirror\", 500, count=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.2.5**: There's another use of the covariance matrix, other than just learning how features co-vary. In fact, it turns out that the *eigenvectors* of the covariance matrix are a set of mutually orthogonal vectors, that point in the directions of greatest variance in the data. The eigenvector with the greatest *eigenvalue* points along the direction of greatest variation, and so on. This is pretty neat, because if we know along which axes the data is most stretched, we can figure out how best to project it when visualizing it in 2D as a scatter plot! This whole procedure has a name: **Principal Component Analysis** (PCA) and it was invented by Karl Pearson in 1901. It belongs to a powerful class of linear algebra methods called **Matrix Factorization** methods. Ok, so rather than spending too much time on the math of PCA, let's just use the `sklearn` implementation and fit a PCA on `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T17:02:34.569917Z",
     "start_time": "2019-09-23T17:02:34.555036Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=10, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">1. Explain what the matrix you get when you call `pca.components_` means.\n",
    "2. Make a bar plot of `pca.explained_variance_ratio_` and explain what it means (you may want to log-scale the y-axis). What insights about our data can we extract from this?\n",
    "3. Indeed, problem with the data AS-IS, is that the different features have very different variances (some are huge numbers others are small). The way to fix this is by doing something called \"[zscoring](https://en.wikipedia.org/wiki/Standard_score)\", whereby each feature is rescaled to have zero mean and unit standard deviation. In this way, all of the data ends up with comparable variance. Make a new array `X_z` that is the zscored `X`, using the `scipy.stats.zscore` function. Show that each column has zero mean and unit standard deviation.\n",
    "4. Transform `X` using the PCA we fitted above to create a new array `X_pca`. Then fit a new PCA to `X_z` and transform it to create another new array `X_z_pca`. Finally, scatter plot against each other the first two components (i.e. fitst two columns in array) of `X_pca`. Do the same for `X_z_pc`. Comment on the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.3.1**: Take a vector `a = [1, 3, 2, 5, 3, 1, 5, 1, 9000]`:\n",
    "1. Compute the mean of `a` using `numpy`.\n",
    "2. How is median defined? Compute the median of `a` using `numpy`.\n",
    "3. For `a`, why might it make sense to take the median more seriously than the mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 3.1.1**: From the Wikipedia API, get a list of all Marvel superheroes and another list of all Marvel supervillains. Use the `get_categorymembers` function below to get the characters in each category: 'Category:Marvel_Comics_supervillains' and 'Category:Marvel_Comics_superheroes'. Make sure you spend some time understanding the code.  How is the query formed?  Why does it take that form?  It will help to look at the [Categorymembers API](https://www.mediawiki.org/wiki/API:Categorymembers).  Moreoever, understand the need for the while loop and role played by the `cmcontinue` variable and query argument.\n",
    "\n",
    ">After you've obtained the lists for superheroes and supervillains, write some code to answer:\n",
    "1. How many characters are *ambiguous*, i.e. are both heroes and villains? What is the Jaccard similarity between the two groups?\n",
    "2. How many superheroes are there? How many supervillains? Do not include ambiguous characters in these counts!\n",
    ">\n",
    ">*Hint: Google something like \"get list all pages in category wikimedia api\" if you're struggling with the query. Also, you may notice that not only Marvel character pages are returned, but also names of subcategories. For now just ignore this and treat them as if they are also characters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this function that gets the categorymembers of a category\n",
    "def get_categorymembers(category):\n",
    "    members = []\n",
    "    cmcontinue = \"\"\n",
    "    while True:\n",
    "\n",
    "        # Download data\n",
    "        data = rq.get('https://en.wikipedia.org/w/api.php?format=json&action=query&list=categorymembers&cmtitle=%s&cmlimit=max&cmcontinue=%s' % (category, cmcontinue)).json()    \n",
    "        #print(data)\n",
    "        \n",
    "        # Add member titles\n",
    "        members.extend(\n",
    "            [m['title'] for m in data['query']['categorymembers']]\n",
    "        )\n",
    "\n",
    "        # If there is a 'continue' key in `data` then fetch the next 'cmcontinue' value\n",
    "        if 'continue' in data:\n",
    "            cmcontinue = data['continue']['cmcontinue']\n",
    "\n",
    "        # Otherwise break\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return members"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 3.2.1**: Extract the length of the page of each character (once you open the correponsing file) and plot the distribution of this variable for each class (heroes/villains/ambiguous). Can you say anything about the popularity of characters in the Marvel universe based on your visualization?\n",
    ">\n",
    ">*Hint: The simplest thing is to make a probability mass function, i.e. a normalized histogram. [My figure](https://ulfaslak.com/computational_analysis_of_big_data/exer_figures/example_3.2.1.png) looks like this. Use `plt.hist` on a list of page lengths, with the argument `density=True`. Other distribution plots are fine too, though.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 3.2.3**: In this exercise you want to find out the biggest alliances in the Marvel universe and their members. The data that will help you in doing this is in the *alliances*-field of the markup of each character -- open up a couple of character files and look for that field; get a sense for how the information is stored so that you can then write code to retreave it. Below I suggest steps you can take to solve the problem if you get stuck.\n",
    "* Use the regular expression `alliances[\\w\\W]+?\\n` to extract the *alliances*-field of a character's markup.\n",
    "* Use the regular expression `\\[\\[.+?[\\]\\|]` to extract links (i.e. each team) from the *alliance*-field.\n",
    "* You want to store alliance names and the corresponding members (hint: use a `defaultdict`).\n",
    "* Inspect your team names. Are there any that result from inconsistencies in the information on the pages? How do you deal with this?\n",
    "* **Print the 10 largest alliances and their number of members.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
